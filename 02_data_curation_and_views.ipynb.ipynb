{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2775957-59ee-4249-880c-12a680169947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Novo Notebook (ex: 03_tft_data_analysis.ipynb) - Célula 1: Configuração SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"Iniciando SparkSession para novo notebook...\")\n",
    "\n",
    "# A SparkSession já é fornecida e configurada pelo Databricks.\n",
    "# Ela já herdará o acesso ao S3 via IAM Role do seu CloudFormation.\n",
    "spark = SparkSession.builder.appName(\"TFTDataAnalysis\").getOrCreate()\n",
    "\n",
    "print(\"SparkSession configurada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b733b12d-cdb4-4514-ab71-d196330141d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Novo Notebook - Célula 2: Consultando a tabela do Unity Catalog\n",
    "\n",
    "# Nome completo da sua tabela no Unity Catalog\n",
    "UNITY_CATALOG_TABLE_NAME = \"tft_analytics.raw.tft_matches_raw\"\n",
    "\n",
    "print(f\"Lendo dados da tabela '{UNITY_CATALOG_TABLE_NAME}'...\")\n",
    "\n",
    "# Opção 1: Usar Spark SQL (recomendado para consultas diretas)\n",
    "df_raw_from_catalog_sql = spark.sql(f\"SELECT * FROM {UNITY_CATALOG_TABLE_NAME}\")\n",
    "print(\"\\nDados da tabela via Spark SQL (primeiras 5 linhas):\")\n",
    "df_raw_from_catalog_sql.show(5, truncate=False)\n",
    "\n",
    "# Opção 2: Carregar como DataFrame PySpark (para manipulação posterior com Python)\n",
    "df_raw_from_catalog_pyspark = spark.read.table(UNITY_CATALOG_TABLE_NAME)\n",
    "print(\"\\nDados da tabela via PySpark (esquema):\")\n",
    "df_raw_from_catalog_pyspark.printSchema()\n",
    "\n",
    "print(f\"\\nTotal de registros na tabela '{UNITY_CATALOG_TABLE_NAME}': {df_raw_from_catalog_pyspark.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "471c6dc2-03e6-49de-84b1-ad6d3d4c3aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Célula para Criar o Esquema 'curated'\n",
    "\n",
    "CATALOG_NAME = \"tft_analytics\"\n",
    "CURATED_SCHEMA_NAME = \"curated\"\n",
    "\n",
    "print(f\"Verificando e criando o esquema '{CATALOG_NAME}.{CURATED_SCHEMA_NAME}'...\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{CURATED_SCHEMA_NAME}\").show()\n",
    "\n",
    "print(f\"Esquema '{CATALOG_NAME}.{CURATED_SCHEMA_NAME}' criado ou já existente.\")\n",
    "\n",
    "# Opcional: Conceder permissões se necessário (geralmente você já tem como criador)\n",
    "# spark.sql(f\"GRANT CREATE TABLE ON SCHEMA {CATALOG_NAME}.{CURATED_SCHEMA_NAME} TO `users`\").show()\n",
    "# spark.sql(f\"GRANT USE SCHEMA ON SCHEMA {CATALOG_NAME}.{CURATED_SCHEMA_NAME} TO `users`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ace5be9-ff4b-41bd-9345-a0f095e9c900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Célula para Criar uma View SQL\n",
    "# Lembre-se que o nome da sua tabela é catalog.schema.table\n",
    "UNITY_CATALOG_TABLE_NAME = \"tft_analytics.raw.tft_matches_raw\"\n",
    "VIEW_NAME = \"tft_analytics.curated.match_summary_view\" # Nome da sua nova view\n",
    "\n",
    "print(f\"Criando a view '{VIEW_NAME}' a partir da tabela '{UNITY_CATALOG_TABLE_NAME}'...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {VIEW_NAME} AS\n",
    "    SELECT\n",
    "        metadata.match_id,\n",
    "        info.game_version,\n",
    "        info.game_length,\n",
    "        info.queue_id,\n",
    "        info.tft_set_number\n",
    "    FROM {UNITY_CATALOG_TABLE_NAME}\n",
    "\"\"\").show(truncate=False) # .show() aqui é apenas para exibir o status da operação, não os dados da view\n",
    "\n",
    "print(f\"\\nView '{VIEW_NAME}' criada com sucesso!\")\n",
    "print(\"Agora você pode consultar a view como se fosse uma tabela.\")\n",
    "\n",
    "# Testando a nova view\n",
    "print(f\"\\nConsultando as primeiras 5 linhas da view '{VIEW_NAME}':\")\n",
    "spark.sql(f\"SELECT * FROM {VIEW_NAME} LIMIT 5\").show(truncate=False)\n",
    "\n",
    "# Exemplo de consulta mais complexa na view\n",
    "print(f\"\\nContando partidas por versão do jogo usando a view '{VIEW_NAME}':\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        game_version,\n",
    "        COUNT(match_id) AS total_matches\n",
    "    FROM {VIEW_NAME}\n",
    "    GROUP BY game_version\n",
    "    ORDER BY total_matches DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_data_curation_and_views.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
