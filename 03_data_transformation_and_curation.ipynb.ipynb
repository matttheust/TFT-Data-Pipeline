{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1bd257-cd59-4aea-bacb-597d2a59cbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03_data_transformation_and_curation.ipynb - Célula 1: Configuração da SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, lit, expr, from_unixtime, to_timestamp, array_contains, when, concat_ws, to_date, sum as spark_sum, count as spark_count, avg as spark_avg\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso.\")\n",
    "\n",
    "# A SparkSession já é automaticamente fornecida e configurada pelo Databricks.\n",
    "# Ela já herda o acesso ao S3 via a IAM Role do seu CloudFormation.\n",
    "spark = SparkSession.builder.appName(\"TFTDataTransformationAndCuratio\").getOrCreate()\n",
    "\n",
    "print(\"SparkSession configurada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1f8766-8153-4c0b-9bc0-b63bb2750793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03_data_transformation_and_curation.ipynb - Célula 2: Carregar Dados Brutos do Unity Catalog\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Nome completo da sua tabela no Unity Catalog\n",
    "UNITY_CATALOG_RAW_TABLE_NAME = \"tft_analytics.raw.tft_matches_raw\"\n",
    "\n",
    "print(f\"Carregando dados da tabela '{UNITY_CATALOG_RAW_TABLE_NAME}'...\")\n",
    "\n",
    "# Carrega o DataFrame a partir da tabela do Unity Catalog\n",
    "df_raw_spark = spark.read.table(UNITY_CATALOG_RAW_TABLE_NAME)\n",
    "\n",
    "print(f\"Dados carregados. Total de registros: {df_raw_spark.count()}\")\n",
    "print(\"\\nEsquema inicial dos dados brutos:\")\n",
    "df_raw_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d7df87-cb32-4651-aae8-663f556e06e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03_data_transformation_and_curation.ipynb - Célula 3: Transformação e Achatamento dos Dados (SOLUÇÃO COM selectExpr)\n",
    "\n",
    "from pyspark.sql.functions import col, explode, from_unixtime, to_date, when, concat_ws # 'transform' NÃO será usado diretamente aqui\n",
    "\n",
    "print(\"Iniciando o achatamento e transformação dos dados...\")\n",
    "\n",
    "# 1. Achatar (Flatten) as informações de partida e explodir a lista de participantes\n",
    "df_flattened_participants = df_raw_spark.select(\n",
    "    col(\"metadata.match_id\").alias(\"match_id\"),\n",
    "    from_unixtime(col(\"info.game_datetime\") / 1000).alias(\"game_datetime_utc\"),\n",
    "    to_date(from_unixtime(col(\"info.game_datetime\") / 1000)).alias(\"game_date_utc\"),\n",
    "    col(\"info.game_length\").alias(\"game_length_seconds\"),\n",
    "    col(\"info.game_version\").alias(\"game_version\"),\n",
    "    col(\"info.queue_id\").alias(\"queue_id\"),\n",
    "    col(\"info.tft_set_core_name\").alias(\"tft_set_name\"),\n",
    "    col(\"info.tft_set_number\").alias(\"tft_set_number\"),\n",
    "    explode(col(\"info.participants\")).alias(\"participant_data\")\n",
    ")\n",
    "\n",
    "# 2. Extrair detalhes específicos de cada participante e usar selectExpr para transformar arrays\n",
    "# USAMOS SELECTEXPR AQUI PARA TODAS AS COLUNAS, INCLUINDO AS TRANSFORMAÇÕES DE ARRAY\n",
    "df_transformed = df_flattened_participants.selectExpr(\n",
    "    \"match_id\",\n",
    "    \"game_datetime_utc\",\n",
    "    \"game_date_utc\",\n",
    "    \"game_length_seconds\",\n",
    "    \"game_version\",\n",
    "    \"queue_id\",\n",
    "    \"tft_set_name\",\n",
    "    \"tft_set_number\",\n",
    "    \"participant_data.puuid AS puuid\",\n",
    "    \"participant_data.placement AS placement\",\n",
    "    \"participant_data.gold_left AS gold_left\",\n",
    "    \"participant_data.last_round AS last_round\",\n",
    "    \"participant_data.level AS level\",\n",
    "    \"participant_data.players_eliminated AS players_eliminated\",\n",
    "    \"participant_data.total_damage_to_players AS total_damage_to_players\",\n",
    "    \"participant_data.riotIdGameName AS riot_id_game_name\",\n",
    "    \"participant_data.riotIdTagline AS riot_id_tagline\",\n",
    "    \"participant_data.time_eliminated AS time_eliminated_seconds\",\n",
    "    # SOLUÇÃO PARA ARRAY DE STRUCT COM TRANSFORM: Usar a função SQL TRANSFORM\n",
    "    \"CONCAT_WS(',', TRANSFORM(participant_data.traits, t -> t.name)) AS traits_activated_names\",\n",
    "    \"CONCAT_WS(',', TRANSFORM(participant_data.units, u -> u.character_id)) AS units_used_ids\"\n",
    ")\n",
    "\n",
    "# 3. Adicionar colunas de indicadores de vitória/top 4\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"is_winner\",\n",
    "    when(col(\"placement\") == 1, True).otherwise(False)\n",
    ").withColumn(\n",
    "    \"is_top_4\",\n",
    "    when(col(\"placement\") <= 4, True).otherwise(False)\n",
    ")\n",
    "\n",
    "print(\"\\nTransformação e achatamento concluídos. Schema do DataFrame transformado:\")\n",
    "df_transformed.printSchema()\n",
    "\n",
    "# --- CONTAR TODOS OS REGISTROS TRANSFORMADOS ---\n",
    "print(f\"Total de registros (participantes) no DataFrame transformado: {df_transformed.count()}\")\n",
    "# -----------------------------------------------\n",
    "\n",
    "print(\"\\nExemplo das primeiras 10 linhas do DataFrame transformado:\")\n",
    "display(df_transformed.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a20b459-b8b1-4294-a3da-bfd03ffce661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total de registros (participantes) no DataFrame transformado: {df_transformed.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d32bc2-7143-488e-a3ae-dbe0051f6695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03_data_transformation_and_curation.ipynb - Célula 4: Salvar Dados Transformados como Tabela Delta Lake\n",
    "\n",
    "# Você já tem o esquema 'curated' criado no 'tft_analytics' Catalog.\n",
    "# Se não criou ainda, volte e crie o esquema 'tft_analytics.curated' no Unity Catalog.\n",
    "\n",
    "# SUBSTITUA PELO NOME REAL E EXATO DO SEU BUCKET S3\n",
    "S3_BUCKET_NAME = \"tft-data-pipeline-towwers\" # O mesmo bucket usado para os dados raw\n",
    "\n",
    "# Caminho de destino no S3 para a tabela Delta transformados\n",
    "# Criaremos uma subpasta 'curated/tft_player_performance_delta/' no seu bucket\n",
    "S3_CURATED_DELTA_PATH = f\"s3a://{S3_BUCKET_NAME}/curated/tft_player_performance_delta/\"\n",
    "\n",
    "# Nome completo da tabela Delta no Unity Catalog (catalog.schema.table)\n",
    "UNITY_CATALOG_DELTA_TABLE_NAME = \"tft_analytics.curated.tft_player_performance_delta\"\n",
    "\n",
    "print(f\"Iniciando gravação do DataFrame transformado como tabela Delta em: {S3_CURATED_DELTA_PATH}...\")\n",
    "\n",
    "try:\n",
    "    # Salva o DataFrame df_transformed como uma tabela Delta no S3\n",
    "    # O modo 'overwrite' substitui os dados existentes na pasta.\n",
    "    df_transformed.write.format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .save(S3_CURATED_DELTA_PATH)\n",
    "    print(f\"\\nDados transformados salvos com sucesso como arquivos Delta em: {S3_CURATED_DELTA_PATH}\")\n",
    "\n",
    "    # Cria (ou recria) a Tabela Externa no Unity Catalog apontando para o Delta Lake\n",
    "    print(f\"Tentando criar/recriar tabela externa '{UNITY_CATALOG_DELTA_TABLE_NAME}' no Unity Catalog...\")\n",
    "    \n",
    "    # Registra a tabela Delta no Unity Catalog usando o caminho do S3.\n",
    "    df_transformed.write.format(\"delta\") \\\n",
    "                        .option(\"path\", S3_CURATED_DELTA_PATH) \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .saveAsTable(UNITY_CATALOG_DELTA_TABLE_NAME)\n",
    "\n",
    "    print(f\"Tabela externa '{UNITY_CATALOG_DELTA_TABLE_NAME}' criada/atualizada com sucesso no Unity Catalog!\")\n",
    "\n",
    "    # Verificação rápida da tabela Delta recém-criada\n",
    "    print(\"\\nVerificação (query SQL na tabela Delta):\")\n",
    "    spark.sql(f\"SELECT COUNT(*) FROM {UNITY_CATALOG_DELTA_TABLE_NAME}\").show()\n",
    "    spark.sql(f\"SELECT * FROM {UNITY_CATALOG_DELTA_TABLE_NAME} LIMIT 5\").show(truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRO ao salvar dados ou criar tabela Delta: {e}\")\n",
    "    print(\"Verifique:\")\n",
    "    print(f\"  - O nome do bucket S3 '{S3_BUCKET_NAME}' está correto e exato.\")\n",
    "    print(f\"  - As permissões da IAM Role permitem escrita no S3 para o caminho '{S3_CURATED_DELTA_PATH}'.\")\n",
    "    print(f\"  - Se você tem as permissões 'CREATE TABLE' e 'USE SCHEMA' no esquema 'tft_analytics.curated'.\")\n",
    "    print(f\"  - Detalhes do erro: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_data_transformation_and_curation.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
